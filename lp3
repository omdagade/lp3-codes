DAA-A1

 

import time
def fiborecursive(n2):
    if n2==0:
        return 0
    elif n2==1:
        return 1
    else:
        return fiborecursive(n2-1)+fiborecursive(n2-2)

def fiboiterative(n1):
    a=0
    b=1
    sum=a+b
    count=1
    while(count<=n1):
        count+=1
        print(a,end=" ")
        a=b
        b=sum
        sum=a+b


n=int(input("Enter Number-: "))
if(n<1):
    print("PLZ ENTER A VALID NUMBER")
x=time.time()
fiboiterative(n)
time.sleep(1)
y=time.time()

print()

print(y-x-1)
print()

x=time.time()
for i in range(n):
    print(fiborecursive(i),end=" ")
print()
time.sleep(1)
y=time.time()
print(y-x-1)




A2

class Node:


    def __init__(self, freq_, symbol_, left_=None, right_=None):

        self.freq = freq_

        self.symbol = symbol_

        self.left = left_

        self.right = right_

        self.huff = ""


def print_nodes(node, val=""):

    new_val = val + str(node.huff)


    if node.left:
        print_nodes(node.left, new_val)
    if node.right:
        print_nodes(node.right, new_val)

    if not node.left and not node.right:
        print(f"{node.symbol} -> {new_val}")



chars = ["a", "b", "c", "d"]

freq = [5, 1, 6, 3]

nodes = [Node(freq[x], chars[x]) for x in range(len(chars))]

while len(nodes) > 1:

    nodes = sorted(nodes, key=lambda x: x.freq)

    left = nodes[0]
    right = nodes[1]

    left.huff = 0
    right.huff = 1

    newNode = Node(left.freq + right.freq, left.symbol + right.symbol, left, right)


    nodes.remove(left)
    nodes.remove(right)
    nodes.append(newNode)


print("Characters :", f'[{", ".join(chars)}]')
print("Frequency  :", freq, "\n\nHuffman Encoding:")
print_nodes(nodes[0])




OR

import heapq

# Creating Huffman tree node
class node:
    def __init__(self,freq,symbol,left=None,right=None):
        self.freq=freq # frequency of symbol
        self.symbol=symbol # symbol name (character)
        self.left=left # node left of current node
        self.right=right # node right of current node
        self.huff= '' # # tree direction (0/1)

    def __lt__(self,nxt): # Check if curr frequency less than next nodes freq
        return self.freq<nxt.freq

def printnodes(node,val=''):
    newval=val+str(node.huff)
    # if node is not an edge node then traverse inside it
    if node.left: 
        printnodes(node.left,newval)
    if node.right: 
        printnodes(node.right,newval)

    # if node is edge node then display its huffman code
    if not node.left and not node.right:
        print("{} -> {}".format(node.symbol,newval))

if __name__=="__main__":
    chars = ['a', 'b', 'c', 'd', 'e', 'f']
    freq = [ 5, 9, 12, 13, 16, 45]
    nodes=[]    

    for i in range(len(chars)): # converting characters and frequencies into huffman tree nodes
        heapq.heappush(nodes, node(freq[i],chars[i]))

    while len(nodes)>1:
        left=heapq.heappop(nodes)
        right=heapq.heappop(nodes)

        left.huff = 0
        right.huff = 1
        # Combining the 2 smallest nodes to create new node as their parent
        newnode = node(left.freq + right.freq , left.symbol + right.symbol , left , right)
        # node(freq,symbol,left,right)
        heapq.heappush(nodes, newnode)

    printnodes(nodes[0]) # Passing root of Huffman Tree




A4
def knapsack(profits, weights, capacity):
 

    n = len(profits)
    items = [(profits[i], weights[i]) for i in range(n)]
    items.sort(key=lambda x: x[0] / x[1], reverse=True)

    best_value = 0
    best_subset = []

    def backtrack(i, current_value, current_weight, subset):
        nonlocal best_value, best_subset

        if current_weight > capacity:
            return

        if i == n:
            if current_value > best_value:
                best_value = current_value
                best_subset = subset[:]
            return

        backtrack(i + 1, current_value, current_weight, subset)

        profit, weight = items[i]
        if current_weight + weight <= capacity:
            subset.append(i)
            backtrack(i + 1, current_value + profit, current_weight + weight, subset)
            subset.pop()

    backtrack(0, 0, 0, [])
    return best_value, best_subset

if __name__ == "__main__":
    weights = [1, 3, 5, 7]
    profits = [2, 4, 7, 10]
    capacity = 8

    best_value, best_subset = knapsack(profits, weights, capacity)
    print("Best value:", best_value)
    print("Best subset:", best_subset)



A5


global N
N = 4

def printSolution(board):
    for i in range(N):
        for j in range(N):
            print (board[i][j], end = " ")
        print()

def isSafe(board, row, col):
    for i in range(col):
        if board[row][i] == 1:
            return False

    for i,j in zip(range(row, -1, -1),range(col, -1, -1)):
        if board[i][j] == 1:
            return False

    for i,j in zip(range(row, N, 1), range(col, -1, -1)):
        if board[i][j] == 1:
            return False
    
    return True

def solveNQUtil(board, col):
    if col >= N:
        return True

    for i in range(N):
        if isSafe(board, i, col):
            board[i][col] = 1
            if solveNQUtil(board, col+1) == True:
                return True
            board[i][col] = 0
    return False

def solveNQ():
    board = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]
    if solveNQUtil(board, 0) == False:
        print("Solution does not exist")
        return False
    printSolution(board)
    return True

solveNQ()

 

A6

import random
import time

# Deterministic Quick Sort
def deterministic_quick_sort(arr):
    if len(arr) <= 1:
        return arr
    else:
        pivot = arr[0]
        less = [i for i in arr[1:] if i <= pivot]
        greater = [i for i in arr[1:] if i > pivot]
        return deterministic_quick_sort(less) + [pivot] + deterministic_quick_sort(greater)

# Randomized Quick Sort
def randomized_quick_sort(arr):
    if len(arr) <= 1:
        return arr
    else:
        pivot = random.choice(arr)
        less = [i for i in arr if i < pivot]
        equal = [i for i in arr if i == pivot]
        greater = [i for i in arr if i > pivot]
        return randomized_quick_sort(less) + equal + randomized_quick_sort(greater)

# Function for analysis
def analyze_sorting_algorithm(sort_function, arr):
    start_time = time.time()
    sorted_arr = sort_function(arr)
    end_time = time.time()
    return sorted_arr, end_time - start_time

# Example usage for analysis
arr = [3, 7, 8, 5, 2, 1, 9, 5, 4]
print("Original array:", arr)

sorted_deterministic, time_deterministic = analyze_sorting_algorithm(deterministic_quick_sort, arr)
sorted_randomized, time_randomized = analyze_sorting_algorithm(randomized_quick_sort, arr)

print("Deterministic sorted array:", sorted_deterministic)
print("Time taken by Deterministic Quick Sort:", time_deterministic, "seconds")

print("Randomized sorted array:", sorted_randomized)
print("Time taken by Randomized Quick Sort:", time_randomized, "seconds")




B1

import pandas as pd
import numpy as np
from sklearn import metrics

om=pd.read_csv("uber.csv")
om.head()
om.isnull().sum()
om.dropna(axis=0,inplace=True)
om.isnull().sum()
import datetime
om.pickup_datetime = pd.to_datetime(om.pickup_datetime, errors='coerce') 

om= om.assign(
    second = om.pickup_datetime.dt.second,
    minute = om.pickup_datetime.dt.minute,
    hour = om.pickup_datetime.dt.hour,
    day= om.pickup_datetime.dt.day,
    month = om.pickup_datetime.dt.month,
    year = om.pickup_datetime.dt.year,
    dayofweek = om.pickup_datetime.dt.dayofweek
)
om = om.drop('pickup_datetime',axis=1)

om.info()

incorrect_coordinates = om.loc[
    (om.pickup_latitude > 90) |(om.pickup_latitude < -90) |
    (om.dropoff_latitude > 90) |(om.dropoff_latitude < -90) |
    (om.pickup_longitude > 180) |(om.pickup_longitude < -180) |
    (om.dropoff_longitude > 90) |(om.dropoff_longitude < -90)
]

om.drop(incorrect_coordinates, inplace = True, errors = 'ignore')

 

def distance_transform(longitude1, latitude1, longitude2, latitude2):
    long1, lati1, long2, lati2 = map(np.radians, [longitude1, latitude1, longitude2, latitude2])
    dist_long = long2 - long1
    dist_lati = lati2 - lati1
    a = np.sin(dist_lati/2)**2 + np.cos(lati1) * np.cos(lati2) * np.sin(dist_long/2)**2
    c = 2 * np.arcsin(np.sqrt(a)) * 6371

    return c

   

om['Distance'] = distance_transform(
    om['pickup_longitude'],
    om['pickup_latitude'],
    om['dropoff_longitude'],
    om['dropoff_latitude']
)

 

import matplotlib.pyplot as plt
import seaborn as sns

 

plt.scatter(om['Distance'], om['fare_amount'])
plt.xlabel("Distance")
plt.ylabel("fare_amount")

 

plt.figure(figsize=(20,12))
sns.boxplot(data = om)

 

om.drop(om[om['Distance'] >= 60].index, inplace = True)
om.drop(om[om['fare_amount'] <= 0].index, inplace = True)

om.drop(om[(om['fare_amount']>100) & (om['Distance']<1)].index, inplace = True )
om.drop(om[(om['fare_amount']<100) & (om['Distance']>100)].index, inplace = True )

 

plt.scatter(om['Distance'], om['fare_amount'])
plt.xlabel("Distance")
plt.ylabel("fare_amount")

 

X = om['Distance'].values.reshape(-1, 1)        #Independent Variable
y = om['fare_amount'].values.reshape(-1, 1)

 

from sklearn.preprocessing import StandardScaler
std = StandardScaler()
y_std = std.fit_transform(y)
print(y_std)

x_std = std.fit_transform(X)

print(x_std)

 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x_std, y_std, test_size=0.2, random_state=0)

 

from sklearn.linear_model import LinearRegression
l_reg = LinearRegression()
l_reg.fit(X_train, y_train)

print("Training set score: {:.2f}".format(l_reg.score(X_train, y_train)))
print("Test set score: {:.7f}".format(l_reg.score(X_test, y_test)))

 

y_pred = l_reg.predict(X_test)

result = pd.DataFrame()
result[['Actual']] = y_test
result[['Predicted']] = y_pred

result.sample(10)

 

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Absolute % Error:', metrics.mean_absolute_percentage_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('R Squared (R²):', np.sqrt(metrics.r2_score(y_test, y_pred)))

 

plt.subplot(2, 2, 1)
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, l_reg.predict(X_train), color ="blue")
plt.title("Fare vs Distance (Training Set)")
plt.ylabel("fare_amount")
plt.xlabel("Distance")

plt.subplot(2, 2, 2)
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, l_reg.predict(X_train), color ="blue")
plt.ylabel("fare_amount")
plt.xlabel("Distance")
plt.title("Fare vs Distance (Test Set)")

plt.tight_layout()
plt.show()

 

cols = ['Model', 'RMSE', 'R-Squared']

result_tabulation = pd.DataFrame(columns = cols)

linreg_metrics = pd.DataFrame([[
     "Linear Regresion model",
     np.sqrt(metrics.mean_squared_error(y_test, y_pred)),
     np.sqrt(metrics.r2_score(y_test, y_pred))
]], columns = cols)

result_tabulation = pd.concat([result_tabulation, linreg_metrics], ignore_index=True)

result_tabulation

 

from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor(n_estimators=100, random_state=10)

rf_reg.fit(X_train, y_train)

y_pred_RF = rf_reg.predict(X_test)

result = pd.DataFrame()
result[['Actual']] = y_test
result['Predicted'] = y_pred_RF

result.sample(10)

 

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_RF))
print('Mean Absolute % Error:', metrics.mean_absolute_percentage_error(y_test, y_pred_RF))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_RF))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_RF)))
print('R Squared (R²):', np.sqrt(metrics.r2_score(y_test, y_pred_RF)))

 

plt.scatter(X_test, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
plt.scatter(X_test, y_pred_RF, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
plt.xlabel('Carat')
plt.ylabel('Price')
plt.grid(color = '#D3D3D3', linestyle = 'solid')
plt.legend(loc = 'lower right')

plt.tight_layout()

plt.show()




 

B2

 

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn import preprocessing

 

df = pd.read_csv('emails.csv')

df.drop(columns=['Email No.'], inplace=True)

X=df.iloc[:, :df.shape[1]-1]      
y=df.iloc[:, -1]                   
X.shape, y.shape

 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=8)

 

models = {
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=2),
    "Linear SVM":LinearSVC(random_state=8, max_iter=900000),
    "Polynomical SVM":SVC(kernel="poly", degree=2, random_state=8),
    "RBF SVM":SVC(kernel="rbf", random_state=8),
    "Sigmoid SVM":SVC(kernel="sigmoid", random_state=8)
}

 

for model_name, model in models.items():
    y_pred=model.fit(X_train, y_train).predict(X_test)
    print(f"Accuracy for {model_name} model \t: {metrics.accuracy_score(y_test, y_pred)}")

 

 

 

B4

 

cur_x = 2 # The algorithm starts at x=3 
rate = 0.01 # Learning rate
precision = 0.000001 #This tells us when to stop the algorithm 
previous_step_size = 1 #
max_iters = 10000 # maximum number of iterations
iters = 0 #iteration counter
df = lambda x: 2*(x+5) #Gradient of our function
while previous_step_size > precision and iters < max_iters: 
    prev_x = cur_x #Store current x value in prev_x
    cur_x = cur_x - rate * df(prev_x) #Grad descent 
    previous_step_size = abs(cur_x - prev_x) #Change in x 
    iters = iters+1 #iteration count
    #print("Iteration",iters,"\nX value is",cur_x) #Print iterations
print("The local minimum occurs at", cur_x)

 

 

OR

 

 

from sympy import Symbol, lambdify
import matplotlib.pyplot as plt
import numpy as np

x = Symbol('x')

def gradient_descent(
    function, start, learn_rate, n_iter=10000, tolerance=1e-06, step_size=1
):
    gradient = lambdify(x, function.diff(x))
    function = lambdify(x, function)
    points = [start]
    iters = 0                           #iteration counter
    
    while step_size > tolerance and iters < n_iter:
        prev_x = start                  #Store current x value in prev_x
        start = start - learn_rate * gradient(prev_x) #Grad descent
        step_size = abs(start - prev_x) #Change in x
        iters = iters+1                 #iteration count
        points.append(start)
    print("The local minimum occurs at", start)
    
    # Create plotting array
    x_ = np.linspace(-7,5,100)
    y = function(x_)

    # setting the axes at the centre
    fig = plt.figure(figsize = (10, 10))
    ax = fig.add_subplot(1, 1, 1)
    ax.spines['left'].set_position('center')
    ax.spines['bottom'].set_position('zero')
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')

    # plot the function
    plt.plot(x_,y, 'r')
    plt.plot(points, function(np.array(points)), '-o')

    # show the plot
    plt.show()

function=(x+5)**2

gradient_descent(
     function=function, start=3.0, learn_rate=0.2, n_iter=50
)

 


       
B5

 

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn import preprocessing

 

df = pd.read_csv('diabetes.csv')

 

df.drop(['BloodPressure', 'SkinThickness'], axis=1, inplace=True)

hist = df.hist(figsize=(20,16))

X=df.iloc[:, :df.shape[1]-1]       #Independent Variables
y=df.iloc[:, -1]                   #Dependent Variable
X.shape, y.shape

 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

 

def knn(X_train, X_test, y_train, y_test, neighbors, power):
    model = KNeighborsClassifier(n_neighbors=neighbors, p=power)
    # Fit and predict on model
    # Model is trained using the train set and predictions are made based on the test set. Accuracy scores are calculated for the model.
    y_pred=model.fit(X_train, y_train).predict(X_test)
    print(f"Accuracy for K-Nearest Neighbors model \t: {accuracy_score(y_test, y_pred)}")

    cm = confusion_matrix(y_test, y_pred)
    print(f'''Confusion matrix :\n
    | Positive Prediction\t| Negative Prediction
    ---------------+------------------------+----------------------
    Positive Class | True Positive (TP) {cm[0, 0]}\t| False Negative (FN) {cm[0, 1]}
    ---------------+------------------------+----------------------
    Negative Class | False Positive (FP) {cm[1, 0]}\t| True Negative (TN) {cm[1, 1]}\n''')
    cr = classification_report(y_test, y_pred)
    print('Classification report : \n', cr)

 

OR

 

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, mean_absolute_error, precision_score, f1_score
x = diabetes_data.iloc[:,1:-1]
y = diabetes_data.iloc[:,-1]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=38)
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_pred = knn.predict(x_test)
confusion_matrix(y_test, knn_pred)
print("Accuracy: ", accuracy_score(y_test, knn_pred)*100)
print("Mean Absolute Error: ", mean_absolute_error(y_test, knn_pred))
print("Recall: ", recall_score(y_test, knn_pred))
print("Precision: ", precision_score(y_test, knn_pred))
print("F1 Score: ", f1_score(y_test, knn_pred))
 
B6

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('sales_data_sample.csv', encoding='latin1')
df.head()
df= df[['PRICEEACH', 'MSRP']]
from sklearn.cluster import KMeans

inertia = []

for i in range(1, 11):
    clusters = KMeans(n_clusters=i, init='k-means++', random_state=42)
    clusters.fit(df)
    inertia.append(clusters.inertia_)
    
plt.figure(figsize=(6, 6))
sns.lineplot(x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], y = inertia)

kmeans = KMeans(n_clusters = 3, random_state = 42)
y_kmeans = kmeans.fit_predict(df)
y_kmeans

plt.figure(figsize=(8,8))
sns.scatterplot(x=df['PRICEEACH'], y=df['MSRP'], hue=y_kmeans)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c = 'red', label = 'Centroids', marker='*')
plt.legend()

 

kmeans.cluster_centers_

 

 

BCT C3

// SPDX-License-Identifier: Unlicensed
pragma solidity ^0.6.0;

contract MyBank
{
    mapping(address=> uint ) private _balances;
    address public owner;
    event LogDepositeMade(address accountHoder, uint amount );

    constructor () public
     {
         owner=msg.sender;
         emit LogDepositeMade(msg.sender, 1000);
     }

        function deposite() public payable  returns (uint)
        {

        require ((_balances[msg.sender] + msg.value) >  _balances[msg.sender] && msg.sender!=address(0));
        _balances[msg.sender] += msg.value;
        emit LogDepositeMade(msg.sender , msg.value);
        return _balances[msg.sender];
        } 

        function withdraw (uint withdrawAmount) public  returns (uint)
        {

                require (_balances[msg.sender] >= withdrawAmount);
                require(msg.sender!=address(0));
                require (_balances[msg.sender] > 0);
                _balances[msg.sender]-= withdrawAmount;
                msg.sender.transfer(withdrawAmount);
                emit LogDepositeMade(msg.sender , withdrawAmount);
                return _balances[msg.sender];

        }

        function viewBalance() public view returns (uint)
        {
            return _balances[msg.sender];
        }
   
}

 

 

BCT C4
 

// SPDX-License-Identifier: GPL-3.0
pragma solidity ^0.8;

contract Student_management{
    struct Student{
        int stud_ID;
        string Name;
        string department;
    }
    Student[] Students;
    function add_stud(int stud_ID,string memory Name, string memory department)public {
        Student memory stud = Student(stud_ID,Name,department);
        Students.push(stud);
    }
    function getStudent(int stud_ID)public view returns(string memory, string memory){
        for(uint i = 0; i<Students.length;i++){
            Student memory stud = Students[i];
            if(stud.stud_ID == stud_ID){
                return (stud.Name, stud.department);
            }
        }
        return ("Not Found", "Not Found");
    }
}
